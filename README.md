# KNN-Algorithm

The k-nearest neighbors algorithm, also known as KNN, is a supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.

The k value in the kNN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. 

This project focuses on measuring and improving the performance of a KNN classifier using cross-validation. I used a dataset on diabetes, which contained a variety of health metrics and whether or not a patient had diabetes. The data was split into a 70:30 train:test ratio. Then I used K-Fold cross validation technique to split the entire dataset into 10 folds and used the mean score of the folds as the cross validation accuracy. I iterated through k-nearest neighbors and plotted the cross validation accuracy to determine the k-value that resulted in the highest accuracy/ best model performance.    
